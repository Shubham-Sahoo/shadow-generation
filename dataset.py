import torch
from torch.utils.data import Dataset
from PIL import Image
import os
import numpy as np
from typing import Tuple

class ShadowSynthDataset(Dataset):
    def __init__(self, data_dir: str, size: int = 1000000, image_size: int = 256):
        """
        Load ShadowSynth-1M dataset generated by Blender.
        Expects object images, masks, shadow maps, and light parameters.
        """
        self.data_dir = data_dir
        self.image_size = image_size
        # Count available object images
        self.data = [f for f in os.listdir(data_dir) if f.startswith("object_") and f.endswith(".png")]
        self.size = min(size, len(self.data))
    
    def __len__(self) -> int:
        return self.size
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        # Extract index from filename
        file_name = self.data[idx]
        idx_str = file_name.split('_')[1].split('.')[0]
        
        # Load object image
        img_path = os.path.join(self.data_dir, f"object_{idx_str}.png")
        object_image = Image.open(img_path).convert("RGB").resize((self.image_size, self.image_size))
        object_image = torch.from_numpy(np.array(object_image)).permute(2, 0, 1).float() / 255.0
        
        # Load mask
        mask_path = os.path.join(self.data_dir, f"mask_{idx_str}.png")
        object_mask = Image.open(mask_path).convert("L").resize((self.image_size, self.image_size))
        object_mask = torch.from_numpy(np.array(object_mask)).unsqueeze(0).float() / 255.0
        object_mask = (object_mask > 0.5).float()
        
        # Load light parameters
        light_path = os.path.join(self.data_dir, f"light_{idx_str}.txt")
        with open(light_path, 'r') as f:
            azimuth, elevation, intensity = map(float, f.read().split(','))
        light_params = torch.tensor([azimuth / 360, elevation / 90, intensity / 2.0])
        
        # Load shadow map
        shadow_path = os.path.join(self.data_dir, f"shadow_{idx_str}.png")
        shadow_map = Image.open(shadow_path).convert("L").resize((self.image_size, self.image_size))
        shadow_map = torch.from_numpy(np.array(shadow_map)).unsqueeze(0).float() / 255.0
        
        return object_image, object_mask, light_params, shadow_map